---
title: "Data Science Capstone"
author: "Vicky C"
date: '2022-08-10'
output: html_document
---

## Synopsis
In this markdown, we will import, clean and explore the data set given. Then we'll create files using ngrams and tokenization to use in our prediction model.

## Load the necessary libraries.

```{r, warning=FALSE}
library(tibble)
library(tidytext)
library(dplyr)
library(stringr)
library(stringi)
library(ggplot2)
library(tidyr)
library(knitr)
library(scales)
library(corpus)
library(tm)
library(tmap)
library(RWeka)
library(tokenizers)


```

## Importing the Data
Importing only the 3 files that are in English.

```{r}

usBlogs <- file("en_US.blogs.txt", open="rb")
blogs <- readLines(usBlogs, encoding = "UTF-8", skipNul = TRUE)

usNews <- file("en_US.news.txt", open = "rb")
news <- readLines(usNews, encoding = "UTF-8", skipNul = TRUE)

usTwitter <- file("en_US.twitter.txt", open = "rb")
twitter <- readLines(usTwitter, encoding = "UTF-8", skipNul = TRUE)
```

## Data Exploration

We will explore each data file to better understand what is in them

```{r}

exploreFiles <- data.frame(
                            FileName = c("blogs", "news", "twitter"),
                            FileSize = sapply(list(blogs, news, twitter), function(x){format(object.size(x), "MB")}),
                            t(rbind(sapply(list(blogs, news, twitter), stri_stats_general),
                            Words = sapply(list(blogs, news, twitter), stri_stats_latex)[4,1]))
)

exploreFiles

```


### Sampling the Data
Since the files are so large, we will subset the data to use for our project.

```{r}

set.seed(22222)

sampleBlogs <- sample(blogs, length(blogs)*.1, replace = FALSE)

sampleNews <- sample(news, length(news)*.1, replace = FALSE)

sampleTwitter <- sample(twitter, length(twitter)*.1, replace = FALSE)


```

We will now combine the three files to get one sample file. 

```{r}

newData <- c(sampleBlogs, sampleNews, sampleTwitter)

writeLines(newData, "newData.txt")

exploreSample <- data.frame(
  FileName = c("sampleBlogs","sampleNews","sampleTwitter"),
  FileSize = sapply(list(sampleBlogs, sampleNews, sampleTwitter), function(x){format(object.size(x), "MB")}),
  t(rbind(sapply(list(sampleBlogs, sampleNews, sampleTwitter), stri_stats_general),
          Words = sapply(list(sampleBlogs, sampleNews, sampleTwitter), stri_stats_latex)[4,0])
    )
)

exploreSample

```

### Cleaning the Data

We will create a clean corpus file using the corpus package.

```{r, warning=FALSE}

# Cleaning the Corpus Text Function

cleanData <- function(corpus) {
  # convert to lower case
    corpus <- tm_map(corpus, content_transformer(tolower))
    # remove punctuation
    corpus <- tm_map(corpus, content_transformer(removePunctuation))
    # remove numbers
    corpus <- tm_map(corpus, content_transformer(removeNumbers))
    # remove white spaces
    corpus <- tm_map(corpus, content_transformer(stripWhitespace))
     # convert to plain text
    corpus <- tm_map(corpus, content_transformer(PlainTextDocument))
    # remove stopwords
    corpus <- tm_map(corpus, content_transformer(removeWords), stopwords("english"))
}

```

Now we will Tokenize and Construct the N-Grams

```{r}

newData <- iconv(newData, "UTF-8", "ASCII", sub = "")
newData <- VCorpus(VectorSource(newData))
newData <- cleanData(newData)


### N-Grams function
Ngram <- function(corpus, n) {
    token <- function(x) NGramTokenizer(x, Weka_control(min=n, max=n))
    tdm <- TermDocumentMatrix(corpus, control=list(tokenize=token))
    tdm
}

### Function to calculate the frequency of each word or word combo

Ngramfreq <- function(tdm, freq) {
    tdm.freq <- findFreqTerms(tdm, lowfreq = freq)
    tdm.df <- rowSums(as.matrix(tdm[tdm.freq,]))
    tdm.df <- data.frame(word = names(tdm.df), frequency = tdm.df)
    tdm.df <- tdm.df[order(-tdm.df$frequency),]
    tdm.df
}


### Calculate the ngrams

tdm1gram <- Ngram(newData, 1) %>% 
    removeSparseTerms(0.99)
    
tdm2gram <- Ngram(newData, 2) %>% 
    removeSparseTerms(0.999)

tdm3gram <- Ngram(newData, 3) %>% 
    removeSparseTerms(0.9999)

tdm4gram <- Ngram(newData, 4) %>% 
    removeSparseTerms(0.99999)

### count frequency of n-grams
freq1gram <- Ngramfreq(tdm1gram, 20)
freq2gram <- Ngramfreq(tdm2gram, 20)
freq3gram <- Ngramfreq(tdm3gram, 20)
freq4gram <- Ngramfreq(tdm4gram, 20)

### split n-grams into individual words and into a file

unigram <- data.frame(rows = rownames(freq1gram), count = freq1gram$frequency)
unigram$rows <- as.character(unigram$rows)
uni.split <- strsplit(unigram$rows, split = " ")
unigram <- transform(unigram, first = sapply(uni.split,"[[",1))
unigram <- data.frame(unigram = unigram$first,
                      freq = unigram$count,
                      stringsAsFactors = FALSE)
write.csv(unigram[unigram$freq>1,], "unigram.csv",
          row.names = FALSE)
unigram <- read.csv("unigram.csv",
                    stringsAsFactors = FALSE)
saveRDS(unigram, "unigram.RData")

bigram <- data.frame(rows = rownames(freq2gram), count = freq2gram$frequency)
bigram$rows <- as.character(bigram$rows)
bi.split <- strsplit(bigram$rows, split = " ")
bigram <- transform(bigram, 
                    first = sapply(bi.split,"[[",1),
                    second = sapply(bi.split,"[[",2))
bigram <- data.frame(unigram = bigram$first,
                     bigram = bigram$second,
                      freq = bigram$count,
                      stringsAsFactors = FALSE)
write.csv(bigram[bigram$freq>1,], "bigram.csv",
          row.names = FALSE)
bigram <- read.csv("bigram.csv",
                    stringsAsFactors = FALSE)
saveRDS(bigram, "bigram.RData")

trigram <- data.frame(rows = rownames(freq3gram), count = freq3gram$frequency)
trigram$rows <- as.character(trigram$rows)
tri.split <- strsplit(trigram$rows, split = " ")
trigram <- transform(trigram, 
                    first = sapply(tri.split,"[[",1),
                    second = sapply(tri.split,"[[",2),
                    third = sapply(tri.split,"[[",3))
trigram <- data.frame(unigram = trigram$first,
                     bigram = trigram$second,
                     trigram = trigram$third,
                     freq = trigram$count,
                     stringsAsFactors = FALSE)
write.csv(trigram[trigram$freq>1,], "trigram.csv",
          row.names = FALSE)
trigram <- read.csv("trigram.csv",
                   stringsAsFactors = FALSE)
saveRDS(trigram, "trigram.RData")

quadgram <- data.frame(rows = rownames(freq4gram), count = freq4gram$frequency)
quadgram$rows <- as.character(quadgram$rows)
quad.split <- strsplit(quadgram$rows, split = " ")
quadgram <- transform(quadgram, 
                    first = sapply(quad.split,"[[",1),
                    second = sapply(quad.split,"[[",2))
quadgram <- data.frame(unigram = quadgram$first,
                     bigram = quadgram$second,
                     trigram = quadgram$third,
                     quadgram = quadgram$fourth,
                      freq = quadgram$count,
                      stringsAsFactors = FALSE)
write.csv(quadgram[quadgram$freq>1,], "quadgram.csv",
          row.names = FALSE)
quadgram <- read.csv("quadgram.csv",
                    stringsAsFactors = FALSE)
saveRDS(quadgram, "quadgram.RData")
```
